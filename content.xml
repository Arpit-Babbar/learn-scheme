<content title="(learn scheme)">

<front>
<section title="How to Use This Site">
<subsection title="What is (learn scheme)?">
<lead><brand/> is a tiny, interactive implementation of the <term>Scheme</term> programming language that runs in your browser.</lead>
<p><term>Scheme</term> is a minimalist variant of Lisp that can be studied in order to understand Lisp in general. Lisp is frequently touted as a "powerful" programming language because of its unique design in which code is just data that can manipulated or transformed in order to create new programs at run-time.</p>
</subsection>

<subsection title="How do I use (learn scheme)?">
<p><brand/> is an interactive environment for learning Scheme. At any time, you can click the <term>Launch Editor</term> button in the navigation bar at the top of the screen to launch the editor. Note that, on small screens, you may need to expand the navigation bar in order to find the <term>Launch Editor</term> button.</p>
<p>Once you're in the editor, you can type Scheme code into the <term>Input</term> text box. When you want to execute your code, simply click the <term>Evaluate</term> button or press <term>Ctrl+Enter</term>. The output of the evaluation will be displayed in the <term>Output</term> text box.</p>
<p>To navigate the content on <brand/>, use the Next, Contents, and Previous links at the top and bottom of each page.</p>
</subsection>

<subsection title="Demo">
<p>Below, you'll find a sample of Scheme code in a text box, followed by the expected output of evaluating the code.</p>
<p>If you'd like to try the code out, click the <term>Try it</term> button on the right side of the example code text box or double-click the text box itself to launch the editor with the example code. Once the editor is shown, you can follow the instructions above to evaluate the code and view the output.</p>
<p>As an example, the following code</p>
<code>
(+ 1 2)
</code>
<p>evaluates to:</p>
<result>3</result>
</subsection>

<subsection title="Feedback">
<p>I hope you enjoy using <brand/>! If you have any suggestions or if you run into any issues, please open a new issue on the <a href="https://github.com/jaredkrinke/learn-scheme/issues">GitHub issue tracker</a>.</p>
</subsection>
</section>
</front>

<body>
<section title="Building Abstractions with Procedures">
<quote source="John Locke, An Essay Concerning Human Understanding (1690)">  The acts of the mind, wherein it exerts its power over simple ideas, are chiefly these three: 1. Combining several simple ideas into one compound one, and thus all complex ideas are made.  2. The second is bringing two ideas, whether simple or complex, together, and setting them by one another so as to take a view of them at once, without uniting them into one, by which it gets all its ideas of relations. 3.  The third is separating them from all other ideas that accompany them in their real existence: this is called abstraction, and thus all its general ideas are made.</quote>
<p>We are about to study the idea of a <term>computational process</term>. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called <term>data</term>.  The evolution of a process is directed by a pattern of rules called a <term>program</term>.  People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.</p>
<p>A computational process is indeed much like a sorcerer's idea of a spirit.  It cannot be seen or touched.  It is not composed of matter at all.  However, it is very real.  It can perform intellectual work. It can answer questions.  It can affect the world by disbursing money at a bank or by controlling a robot arm in a factory.  The programs we use to conjure processes are like a sorcerer's spells.  They are carefully composed from symbolic expressions in arcane and esoteric <term>programming languages</term> that prescribe the tasks we want our processes to perform.</p>
<p>A computational process, in a correctly working computer, executes programs precisely and accurately.  Thus, like the sorcerer's apprentice, novice programmers must learn to understand and to anticipate the consequences of their conjuring.  Even small errors (usually called <term>bugs</term> or <term>glitches</term>) in programs can have complex and unanticipated consequences.</p>
<p>Fortunately, learning to program is considerably less dangerous than learning sorcery, because the spirits we deal with are conveniently contained in a secure way.  Real-world programming, however, requires care, expertise, and wisdom.  A small bug in a computer-aided design program, for example, can lead to the catastrophic collapse of an airplane or a dam or the self-destruction of an industrial robot.</p>
<p>Master software engineers have the ability to organize programs so that they can be reasonably sure that the resulting processes will perform the tasks intended.  They can visualize the behavior of their systems in advance.  They know how to structure programs so that unanticipated problems do not lead to catastrophic consequences, and when problems do arise, they can <term>debug</term> their programs.  Well-designed computational systems, like well-designed automobiles or nuclear reactors, are designed in a modular manner, so that the parts can be constructed, replaced, and debugged separately.</p>
<subsection title="Programming in Lisp"><p>We need an appropriate language for describing processes, and we will use for this purpose the programming language Lisp.  Just as our everyday thoughts are usually expressed in our natural language (such as English, French, or Japanese), and descriptions of quantitative phenomena are expressed with mathematical notations, our procedural thoughts will be expressed in Lisp.  Lisp was invented in the late 1950s as a formalism for reasoning about the use of certain kinds of logical expressions, called <term>recursion equations</term>, as a model for computation.  The language was conceived by John McCarthy and is based on his paper "Recursive Functions of Symbolic Expressions and Their Computation by Machine" (McCarthy 1960).</p>
<p>Despite its inception as a mathematical formalism, Lisp is a practical programming language.  A Lisp <term>interpreter</term> is a machine that carries out processes described in the Lisp language.  The first Lisp interpreter was implemented by McCarthy with the help of colleagues and students in the Artificial Intelligence Group of the MIT Research Laboratory of Electronics and in the MIT Computation Center.<footnote><p>The <term>Lisp 1 Programmer's Manual</term> appeared in 1960, and the <term>Lisp 1.5 Programmer's Manual</term> (McCarthy 1965) was published in 1962.  The early history of Lisp is described in McCarthy 1978.   </p>
</footnote> Lisp, whose name is an acronym for LISt Processing, was designed to provide symbol-manipulating capabilities for attacking programming problems such as the symbolic differentiation and integration of algebraic expressions.  It included for this purpose new data objects known as atoms and lists, which most strikingly set it apart from all other languages of the period.</p>
<p>Lisp was not the product of a concerted design effort.  Instead, it evolved informally in an experimental manner in response to users' needs and to pragmatic implementation considerations.  Lisp's informal evolution has continued through the years, and the community of Lisp users has traditionally resisted attempts to promulgate any "official" definition of the language.  This evolution, together with the flexibility and elegance of the initial conception, has enabled Lisp, which is the second oldest language in widespread use today (only Fortran is older), to continually adapt to encompass the most modern ideas about program design.  Thus, Lisp is by now a family of dialects, which, while sharing most of the original features, may differ from one another in significant ways.  The dialect of Lisp used in this book is called Scheme.<footnote><p>The two dialects in which most major Lisp programs of the 1970s were written are MacLisp (Moon 1978; Pitman 1983), developed at the MIT Project MAC, and Interlisp (Teitelman 1974), developed at Bolt Beranek and Newman Inc. and the Xerox Palo Alto Research Center.  Portable Standard Lisp (Hearn 1969; Griss 1981) was a Lisp dialect designed to be easily portable between different machines.  MacLisp spawned a number of subdialects, such as Franz Lisp, which was developed at the University of California at Berkeley, and Zetalisp (Moon 1981), which was based on a special-purpose processor designed at the MIT Artificial Intelligence Laboratory to run Lisp very efficiently.  The Lisp dialect used in this book, called Scheme (Steele 1975), was invented in 1975 by Guy Lewis Steele Jr. and Gerald Jay Sussman of the MIT Artificial Intelligence Laboratory and later reimplemented for instructional use at MIT.  Scheme became an IEEE standard in 1990 (IEEE 1990).  The Common Lisp dialect (Steele 1982, Steele 1990) was developed by the Lisp community to combine features from the earlier Lisp dialects to make an industrial standard for Lisp.  Common Lisp became an ANSI standard in 1994 (ANSI 1994).   </p>
</footnote></p>
<p>Because of its experimental character and its emphasis on symbol manipulation, Lisp was at first very inefficient for numerical computations, at least in comparison with Fortran.  Over the years, however, Lisp compilers have been developed that translate programs into machine code that can perform numerical computations reasonably efficiently.  And for special applications, Lisp has been used with great effectiveness.<footnote><p>One such special application was a breakthrough computation of scientific importance -- an integration of the motion of the Solar System that extended previous results by nearly two orders of magnitude, and demonstrated that the dynamics of the Solar System is chaotic.  This computation was made possible by new integration algorithms, a special-purpose compiler, and a special-purpose computer all implemented with the aid of software tools written in Lisp (Abelson et al. 1992; Sussman and Wisdom 1992).   </p>
</footnote>  Although Lisp has not yet overcome its old reputation as hopelessly inefficient, Lisp is now used in many applications where efficiency is not the central concern.  For example, Lisp has become a language of choice for operating-system shell languages and for extension languages for editors and computer-aided design systems.</p>
<p>If Lisp is not a mainstream language, why are we using it as the framework for our discussion of programming?  Because the language possesses unique features that make it an excellent medium for studying important programming constructs and data structures and for relating them to the linguistic features that support them.  The most significant of these features is the fact that Lisp descriptions of processes, called <term>procedures</term>, can themselves be represented and manipulated as Lisp data.  The importance of this is that there are powerful program-design techniques that rely on the ability to blur the traditional distinction between "passive" data and "active" processes.  As we shall discover, Lisp's flexibility in handling procedures as data makes it one of the most convenient languages in existence for exploring these techniques.  The ability to represent procedures as data also makes Lisp an excellent language for writing programs that must manipulate other programs as data, such as the interpreters and compilers that support computer languages.  Above and beyond these considerations, programming in Lisp is great fun.</p>
</subsection>

<section title="The Elements of Programming">
<p>A powerful programming language is more than just a means for instructing a computer to perform tasks.  The language also serves as a framework within which we organize our ideas about processes.  Thus, when we describe a language, we should pay particular attention to the means that the language provides for combining simple ideas to form more complex ideas.  Every powerful language has three mechanisms for accomplishing this:</p>
<ul>
<li><term>primitive expressions</term>, which represent the simplest entities the language is concerned with,  </li>
<li><term>means of combination</term>, by which compound elements are built from simpler ones, and  </li>
<li><term>means of abstraction</term>, by which compound elements can be named and manipulated as units.  </li>
</ul>
<p>In programming, we deal with two kinds of elements: procedures and data. (Later we will discover that they are really not so distinct.) Informally, data is "stuff" that we want to manipulate, and procedures are descriptions of the rules for manipulating the data. Thus, any powerful programming language should be able to describe primitive data and primitive procedures and should have methods for combining and abstracting procedures and data.</p>
<p>In this chapter we will deal only with simple numerical data so that we can focus on the rules for building procedures.<footnote><p>The characterization of numbers as "simple data" is a barefaced bluff. In fact, the treatment of numbers is one of the trickiest and most confusing aspects of any programming language.  Some typical issues involved are these: Some computer systems distinguish <term>integers</term>, such as 2, from <term>real numbers</term>, such as 2.71.  Is the real number 2.00 different from the integer 2? Are the arithmetic operations used for integers the same as the operations used for real numbers?  Does 6 divided by 2 produce 3, or 3.0?  How large a number can we represent? How many decimal places of accuracy can we represent?  Is the range of integers the same as the range of real numbers?  Above and beyond these questions, of course, lies a collection of issues concerning roundoff and truncation errors -- the entire science of numerical analysis.  Since our focus in this book is on large-scale program design rather than on numerical techniques, we are going to ignore these problems.  The numerical examples in this chapter will exhibit the usual roundoff behavior that one observes when using arithmetic operations that preserve a limited number of decimal places of accuracy in noninteger operations.   </p>
</footnote> In later chapters we will see that these same rules allow us to build procedures to manipulate compound data as well.</p>
<section title="Expressions">
<p>One easy way to get started at programming is to examine some typical interactions with an interpreter for the Scheme dialect of Lisp. Imagine that you are sitting at a computer terminal.  You type an <term>expression</term>, and the interpreter responds by displaying the result of its <term>evaluating</term> that expression.</p>
<p>One kind of primitive expression you might type is a number.  (More precisely, the expression that you type consists of the numerals that represent the number in base 10.)  If you present Lisp with a number</p>
<code>486</code>
<p>the interpreter will respond by printing<footnote><p>Throughout this book, when we wish to emphasize the distinction between the input typed by the user and the response printed by the interpreter, we will show the latter in a block quote.   </p>
</footnote></p>
<result>486</result>
<p>Expressions representing numbers may be combined with an expression representing a primitive procedure (such as <code>+</code> or <code>*</code>) to form a compound expression that represents the application of the procedure to those numbers.  For example:</p>
<code>(+ 137 349)</code>
<result>486</result>
<code>(- 1000 334)</code>
<result>666</result>
<code>(* 5 99)</code>
<result>495</result>
<code>(/ 10 5)</code>
<result>2</result>
<code>(+ 2.7 10)</code>
<result>12.7</result>
<p>Expressions such as these, formed by delimiting a list of expressions within parentheses in order to denote procedure application, are called <term>combinations</term>.  The leftmost element in the list is called the <term>operator</term>, and the other elements are called <term>operands</term>.  The value of a combination is obtained by applying the procedure specified by the operator to the <term>arguments</term> that are the values of the operands.</p>
<p>The convention of placing the operator to the left of the operands is known as <term>prefix notation</term>, and it may be somewhat confusing at first because it departs significantly from the customary mathematical convention.  Prefix notation has several advantages, however.  One of them is that it can accommodate procedures that may take an arbitrary number of arguments, as in the following examples:</p>
<code>(+ 21 35 12 7)</code>
<result>75</result>
<code>(* 25 4 12)</code>
<result>1200</result>
<p>No ambiguity can arise, because the operator is always the leftmost element and the entire combination is delimited by the parentheses.</p>
<p>A second advantage of prefix notation is that it extends in a straightforward way to allow combinations to be <term>nested</term>, that is, to have combinations whose elements are themselves combinations:</p>
<code>(+ (* 3 5) (- 10 6))</code>
<result>19</result>
<p>There is no limit (in principle) to the depth of such nesting and to the overall complexity of the expressions that the Lisp interpreter can evaluate. It is we humans who get confused by still relatively simple expressions such as</p>
<code>(+ (* 3 (+ (* 2 4) (+ 3 5))) (+ (- 10 7) 6))</code>
<p>which the interpreter would readily evaluate to be 57.  We can help ourselves by writing such an expression in the form</p>
<code>(+ (* 3
      (+ (* 2 4)
         (+ 3 5)))
   (+ (- 10 7)
      6))</code>
<p>following a formatting convention known as <term>pretty-printing</term>, in which each long combination is written so that the operands are aligned vertically.  The resulting indentations display clearly the structure of the expression.<footnote><p>Lisp systems typically provide features to aid the user in formatting expressions.  Two especially useful features are one that automatically indents to the proper pretty-print position whenever a new line is started and one that highlights the matching left parenthesis whenever a right parenthesis is typed.   </p>
</footnote></p>
<p>Even with complex expressions, the interpreter always operates in the same basic cycle: It reads an expression from the terminal,  evaluates the expression, and prints the result. This mode of operation is often expressed by saying that the interpreter runs in a <term>read-eval-print loop</term>. Observe in particular that it is not necessary to explicitly instruct the interpreter to print the value of the expression.<footnote><p>Lisp obeys the convention that every expression has a value. This convention, together with the old reputation of Lisp as an inefficient language, is the source of the quip by Alan Perlis (paraphrasing Oscar Wilde) that "Lisp programmers know the value of everything but the cost of nothing."   </p>
</footnote></p>
</section>
<section title="Naming and the Environment">
<p>A critical aspect of a programming language is the means it provides for using names to refer to computational objects.  We say that the name identifies a <term>variable</term> whose <term>value</term> is the object.</p>
<p>In the Scheme dialect of Lisp, we name things with <code>define</code>.  Typing</p>
<code>(define size 2)</code>
<p>causes the interpreter to associate the value 2 with the name <code>size</code>.<footnote><p>In this book, we do not show the interpreter's response to evaluating definitions, since this is highly implementation-dependent.   </p>
</footnote> Once the name <code>size</code> has been associated with the number 2, we can  refer to the value 2 by name:</p>
<code><hidden>(define size 2)</hidden>size</code>
<result>2</result>
<code><hidden>(define size 2)</hidden>(* 5 size)</code>
<result>10</result>
<p>Here are further examples of the use of <code>define</code>:</p>
<code>(define pi 3.14159)
(define radius 10)
(* pi (* radius radius))</code>
<result>314.159</result>
<code><hidden>(define pi 3.14159)
(define radius 10)</hidden>(define circumference (* 2 pi radius))
circumference</code>
<result>62.8318</result>
<p><code>Define</code> is our language's simplest means of abstraction, for it allows us to use simple names to refer to the results of compound operations, such as the <code>circumference</code> computed above. In general, computational objects may have very complex structures, and it would be extremely inconvenient to have to remember and repeat their details each time we want to use them.  Indeed, complex programs are constructed by building, step by step, computational objects of increasing complexity. The interpreter makes this step-by-step program construction particularly convenient because name-object associations can be created incrementally in successive interactions.  This feature encourages the incremental development and testing of programs and is largely responsible for the fact that a Lisp program usually consists of a large number of relatively simple procedures.</p>
<p>It should be clear that the possibility of associating values with symbols and later retrieving them means that the interpreter must maintain some sort of memory that keeps track of the name-object pairs.  This memory is called the <term>environment</term> (more precisely the <term>global environment</term>, since we will see later that a computation may involve a number of different environments).<footnote><p>Chapter 3 will show that this notion of environment is crucial, both for understanding how the interpreter works and for implementing interpreters.   </p>
</footnote></p>
</section>
<section title="Evaluating Combinations">
<p>One of our goals in this chapter is to isolate issues about thinking procedurally.  As a case in point, let us consider that, in evaluating combinations, the interpreter is itself following a procedure.</p>
<ul>
<li>To evaluate a combination, do the following: </li>
</ul>
<p>1.  Evaluate the subexpressions of the combination.</p>
<p>2.  Apply the procedure that is the value of the leftmost  subexpression (the operator) to the arguments that are the values of the other subexpressions (the operands). </p>
<p>Even this simple rule illustrates some important points about processes in general.  First, observe that the first step dictates that in order to accomplish the evaluation process for a combination we must first perform the evaluation process on each element of the combination.  Thus, the evaluation rule is <term>recursive</term> in nature; that is, it includes, as one of its steps, the need to invoke the rule itself.<footnote><p>It may seem strange that the evaluation rule says, as part of the first step, that we should evaluate the leftmost element of a combination, since at this point that can only be an operator such as <code>+</code> or <code>*</code> representing a built-in primitive procedure such as addition or multiplication.  We will see later that it is useful to be able to work with combinations whose operators are themselves compound expressions.   </p>
</footnote></p>
<p>Notice how succinctly the idea of recursion can be used to express what, in the case of a deeply nested combination, would otherwise be viewed as a rather complicated process.  For example, evaluating</p>
<code>(* (+ 2 (* 4 6))
   (+ 3 5 7))</code>
<p>requires that the evaluation rule be applied to four different combinations.  We can obtain a picture of this process by representing the combination in the form of a tree, as shown in figure 1.1.  Each combination is represented by a node with branches corresponding to the operator and the operands of the combination stemming from it. The terminal nodes (that is, nodes with no branches stemming from them) represent either operators or numbers. Viewing evaluation in terms of the tree, we can imagine that the values of the operands percolate upward, starting from the terminal nodes and then combining at higher and higher levels.  In general, we shall see that recursion is a very powerful technique for dealing with hierarchical, treelike objects.  In fact, the "percolate values upward" form of the evaluation rule is an example of a general kind of process known as <term>tree accumulation</term>.</p>

<figure image="ch1-Z-G-1.gif"><caption>Tree representation, showing the value of each subcombination.</caption></figure>
<p>Next, observe that the repeated application of the first step brings us to the point where we need to evaluate, not combinations, but primitive expressions such as numerals, built-in operators, or other names.  We take care of the primitive cases by stipulating that</p>
<ul>
<li>the values of numerals are the numbers that they name,  </li>
<li>the values of built-in operators are the machine instruction sequences that carry out the corresponding operations, and  </li>
<li>the values of other names are the objects associated  with those names in the environment. </li>
</ul>
<p>We may regard the second rule as a special case of the third one by stipulating that symbols such as <code>+</code> and <code>*</code> are also included in the global environment, and are associated with the sequences of machine instructions that are their "values."  The key point to notice is the role of the environment in determining the meaning of the symbols in expressions.  In an interactive language such as Lisp, it is meaningless to speak of the value of an expression such as <code>(+ x 1)</code> without specifying any information about the environment that would provide a meaning for the symbol <code>x</code> (or even for the symbol <code>+</code>).  As we shall see in chapter 3, the general notion of the environment as providing a context in which evaluation takes place will play an important role in our understanding of program execution.</p>
<p>Notice that the evaluation rule given above does not handle definitions. For instance, evaluating <code>(define x 3)</code> does not apply <code>define</code> to two arguments, one of which is the value of the symbol <code>x</code> and the other of which is 3, since the purpose of the <code>define</code> is precisely to associate <code>x</code> with a value. (That is, <code>(define x 3)</code> is not a combination.)</p>
<p>Such exceptions to the general evaluation rule are called <term>special forms</term>.  <code>Define</code> is the only example of a special form that we have seen so far, but we will meet others shortly.  Each special form has its own evaluation rule. The various kinds of expressions (each with its associated evaluation rule) constitute the syntax of the programming language.  In comparison with most other programming languages, Lisp has a very simple syntax; that is, the evaluation rule for expressions can be described by a simple general rule together with specialized rules for a small number of special forms.<footnote><p>Special syntactic forms that are simply convenient alternative surface structures for things that can be written in more uniform ways are sometimes called <term>syntactic sugar</term>, to use a phrase coined by Peter Landin.  In comparison with users of other languages, Lisp programmers, as a rule, are less concerned with matters of syntax.  (By contrast, examine any Pascal manual and notice how much of it is devoted to descriptions of syntax.)  This disdain for syntax is due partly to the flexibility of Lisp, which makes it easy to change surface syntax, and partly to the observation that many "convenient" syntactic constructs, which make the language less uniform, end up causing more trouble than they are worth when programs become large and complex.  In the words of Alan Perlis, "Syntactic sugar causes cancer of the semicolon."   </p>
</footnote></p>
</section>
<section title="Compound Procedures">
<p>We have identified in Lisp some of the elements that must appear in any powerful programming language:</p>
<ul>
<li>Numbers and arithmetic operations are  primitive data and procedures.  </li>
<li>Nesting of combinations provides a means of  combining operations.  </li>
<li>Definitions that associate names with values provide a limited means of abstraction. </li>
</ul>
<p>Now we will learn about <term>procedure definitions</term>, a much more powerful abstraction technique by which a compound operation can be given a name and then referred to as a unit.</p>
<p>We begin by examining how to express the idea of "squaring."  We might say, "To square something, multiply it by itself."  This is expressed in our language as </p>
<code>(define (square x) (* x x))</code>
<p>We can understand this in the following way:</p>
<code valid="false">(define (square  x)        (*         x     x))
 To      square something, multiply   it by itself.</code>
<p>We have here a <term>compound procedure</term>, which has been given the name <code>square</code>.  The procedure represents the operation of multiplying something by itself.  The thing to be multiplied is given a local name, <code>x</code>, which plays the same role that a pronoun plays in natural language.  Evaluating the definition creates this compound procedure and associates it with the name <code>square</code>.<footnote><p>Observe that there are two different operations being combined here: we are creating the procedure, and we are giving it the name <code>square</code>.  It is possible, indeed important, to be able to separate these two notions -- to create procedures without naming them, and to give names to procedures that have already been created.  We will see how to do this in section 1.3.2.   </p>
</footnote></p>
<p>The general form of a procedure definition is</p>
<code valid="false">(define (&lt;name&gt; &lt;formal parameters&gt;) &lt;body&gt;)</code>
<p>The &lt;<term>name</term>&gt; is a symbol to be associated with the procedure definition in the environment.<footnote><p>Throughout this book, we will describe the general syntax of expressions by using italic symbols delimited by angle brackets -- e.g., &lt;<term>name</term>&gt; -- to denote the "slots" in the expression to be filled in when such an expression is actually used.   </p>
</footnote> The &lt;<term>formal parameters</term>&gt; are the names used within the body of the procedure to refer to the corresponding arguments of the procedure.  The &lt;<term>body</term>&gt; is an expression that will yield the value of the procedure application when the formal parameters are replaced by the actual arguments to which the procedure is applied.<footnote><p>More generally, the body of the procedure can be a sequence of expressions. In this case, the interpreter evaluates each expression in the sequence in turn and returns the value of the final expression as the value of the procedure application.   </p>
</footnote> The &lt;<term>name</term>&gt; and the &lt;<term>formal parameters</term>&gt; are grouped within parentheses, just as they would be in an actual call to the procedure being defined.</p>
<p>Having defined <code>square</code>, we can now use it:</p>
<code><hidden>(define (square x) (* x x))</hidden>(square 21)</code>
<result>441</result>
<code><hidden>(define (square x) (* x x))</hidden>(square (+ 2 5))</code>
<result>49</result>
<code><hidden>(define (square x) (* x x))</hidden>(square (square 3))</code>
<result>81</result>
<p>We can also use <code>square</code> as a building block in defining other procedures.  For example, <term>x</term><sup>2</sup>  + <term>y</term><sup>2</sup> can be expressed as</p>
<code valid="false">(+ (square x) (square y))</code>
<p>We can easily define a procedure <code>sum-of-squares</code> that, given any two numbers as arguments, produces the sum of their squares:</p>
<code><hidden>(define (square x) (* x x))</hidden>(define (sum-of-squares x y)
  (+ (square x) (square y)))

(sum-of-squares 3 4)</code>
<result>25</result>
<p>Now we can use <code>sum-of-squares</code> as a building block in constructing further procedures:</p>
<code><hidden>(define (square x) (* x x))
(define (sum-of-squares x y)
  (+ (square x) (square y)))</hidden>(define (f a)
  (sum-of-squares (+ a 1) (* a 2)))

(f 5)</code>
<result>136</result>
<p>Compound procedures are used in exactly the same way as primitive procedures.  Indeed, one could not tell by looking at the definition of <code>sum-of-squares</code> given above whether <code>square</code> was built into the interpreter, like <code>+</code> and <code>*</code>, or defined as a compound procedure.</p>
</section>
</section>
</section>
</body>

<back>
<section title="From the Book">
<p>Learn Scheme is an adaptation of <a href="http://mitpress.mit.edu/sicp/"><term>Structure and Interpretation of Computer Programs</term></a> by Harold Abelson and Gerald Jay Sussman with Julie Sussman, second edition, published by the MIT Press.</p>
<p>The content has been reformatted for the Web and made interactive by Jared Krinke. The Scheme interpreter is (c) 2015 Jared Krinke.</p>
<section title="Foreword">
<p>Educators, generals, dieticians, psychologists, and parents program.  Armies, students, and some societies are programmed.  An assault on large problems employs a succession of programs, most of which spring into existence en route.  These programs are rife with issues that appear to be particular to the problem at hand.  To appreciate programming as an intellectual activity in its own right you must turn to computer programming; you must read and write computer programs -- many of them.  It doesn't matter much what the programs are about or what applications they serve.  What does matter is how well they perform and how smoothly they fit with other programs in the creation of still greater programs.  The programmer must seek both perfection of part and adequacy of collection.  In this book the use of "program" is focused on the creation, execution, and study of programs written in a dialect of Lisp for execution on a digital computer.  Using Lisp we restrict or limit not what we may program, but only the notation for our program descriptions.</p>
<p>Our traffic with the subject matter of this book involves us with three foci of phenomena: the human mind, collections of computer programs, and the computer.  Every computer program is a model, hatched in the mind, of a real or mental process.  These processes, arising from human experience and thought, are huge in number, intricate in detail, and at any time only partially understood.  They are modeled to our permanent satisfaction rarely by our computer programs.  Thus even though our programs are carefully handcrafted discrete collections of symbols, mosaics of interlocking functions, they continually evolve: we change them as our perception of the model deepens, enlarges, generalizes until the model ultimately attains a metastable place within still another model with which we struggle.  The source of the exhilaration associated with computer programming is the continual unfolding within the mind and on the computer of mechanisms expressed as programs and the explosion of perception they generate.  If art interprets our dreams, the computer executes them in the guise of programs!</p>
<p>For all its power, the computer is a harsh taskmaster.  Its programs must be correct, and what we wish to say must be said accurately in every detail.  As in every other symbolic activity, we become convinced of program truth through argument.  Lisp itself can be assigned a semantics (another model, by the way), and if a program's function can be specified, say, in the predicate calculus, the proof methods of logic can be used to make an acceptable correctness argument.  Unfortunately, as programs get large and complicated, as they almost always do, the adequacy, consistency, and correctness of the specifications themselves become open to doubt, so that complete formal arguments of correctness seldom accompany large programs.  Since large programs grow from small ones, it is crucial that we develop an arsenal of standard program structures of whose correctness we have become sure -- we call them idioms -- and learn to combine them into larger structures using organizational techniques of proven value.  These techniques are treated at length in this book, and understanding them is essential to participation in the Promethean enterprise called programming.  More than anything else, the uncovering and mastery of powerful organizational techniques accelerates our ability to create large, significant programs.  Conversely, since writing large programs is very taxing, we are stimulated to invent new methods of reducing the mass of function and detail to be fitted into large programs.</p>
<p>Unlike programs, computers must obey the laws of physics.  If they wish to perform rapidly -- a few nanoseconds per state change -- they must transmit electrons only small distances (at most 1 <small><sup>1</sup>/<small>2</small></small> feet).  The heat generated by the huge number of devices so concentrated in space has to be removed.  An exquisite engineering art has been developed balancing between multiplicity of function and density of devices.  In any event, hardware always operates at a level more primitive than that at which we care to program.  The processes that transform our Lisp programs to "machine" programs are themselves abstract models which we program.  Their study and creation give a great deal of insight into the organizational programs associated with programming arbitrary models.  Of course the computer itself can be so modeled.  Think of it: the behavior of the smallest physical switching element is modeled by quantum mechanics described by differential equations whose detailed behavior is captured by numerical approximations represented in computer programs executing on computers composed of ...!</p>
<p>It is not merely a matter of tactical convenience to separately identify the three foci.  Even though, as they say, it's all in the head, this logical separation induces an acceleration of symbolic traffic between these foci whose richness, vitality, and potential is exceeded in human experience only by the evolution of life itself.  At best, relationships between the foci are metastable.  The computers are never large enough or fast enough.  Each breakthrough in hardware technology leads to more massive programming enterprises, new organizational principles, and an enrichment of abstract models.  Every reader should ask himself periodically "Toward what end, toward what end?" -- but do not ask it too often lest you pass up the fun of programming for the constipation of bittersweet philosophy.</p>
<p>Among the programs we write, some (but never enough) perform a precise mathematical function such as sorting or finding the maximum of a sequence of numbers, determining primality, or finding the square root.  We call such programs algorithms, and a great deal is known of their optimal behavior, particularly with respect to the two important parameters of execution time and data storage requirements.  A programmer should acquire good algorithms and idioms.  Even though some programs resist precise specifications, it is the responsibility of the programmer to estimate, and always to attempt to improve, their performance.</p>
<p>Lisp is a survivor, having been in use for about a quarter of a century.  Among the active programming languages only Fortran has had a longer life.  Both languages have supported the programming needs of important areas of application, Fortran for scientific and engineering computation and Lisp for artificial intelligence.  These two areas continue to be important, and their programmers are so devoted to these two languages that Lisp and Fortran may well continue in active use for at least another quarter-century.</p>
<p>Lisp changes.  The Scheme dialect used in this text has evolved from the original Lisp and differs from the latter in several important ways, including static scoping for variable binding and permitting functions to yield functions as values.  In its semantic structure Scheme is as closely akin to Algol 60 as to early Lisps.  Algol 60, never to be an active language again, lives on in the genes of Scheme and Pascal.  It would be difficult to find two languages that are the communicating coin of two more different cultures than those gathered around these two languages.  Pascal is for building pyramids -- imposing, breathtaking, static structures built by armies pushing heavy blocks into place.  Lisp is for building organisms -- imposing, breathtaking, dynamic structures built by squads fitting fluctuating myriads of simpler organisms into place.  The organizing principles used are the same in both cases, except for one extraordinarily important difference: The discretionary exportable functionality entrusted to the individual Lisp programmer is more than an order of magnitude greater than that to be found within Pascal enterprises.  Lisp programs inflate libraries with functions whose utility transcends the application that produced them.  The list, Lisp's native data structure, is largely responsible for such growth of utility.  The simple structure and natural applicability of lists are reflected in functions that are amazingly nonidiosyncratic.  In Pascal the plethora of declarable data structures induces a specialization within functions that inhibits and penalizes casual cooperation.  It is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures.  As a result the pyramid must stand unchanged for a millennium; the organism must evolve or perish.</p>
<p>To illustrate this difference, compare the treatment of material and exercises within this book with that in any first-course text using Pascal.  Do not labor under the illusion that this is a text digestible at MIT only, peculiar to the breed found there.  It is precisely what a serious book on programming Lisp must be, no matter who the student is or where it is used.</p>
<p>Note that this is a text about programming, unlike most Lisp books, which are used as a preparation for work in artificial intelligence.  After all, the critical programming concerns of software engineering and artificial intelligence tend to coalesce as the systems under investigation become larger.  This explains why there is such growing interest in Lisp outside of artificial intelligence.</p>
<p>As one would expect from its goals, artificial intelligence research generates many significant programming problems.  In other programming cultures this spate of problems spawns new languages.  Indeed, in any very large programming task a useful organizing principle is to control and isolate traffic within the task modules via the invention of language.  These languages tend to become less primitive as one approaches the boundaries of the system where we humans interact most often.  As a result, such systems contain complex language-processing functions replicated many times.  Lisp has such a simple syntax and semantics that parsing can be treated as an elementary task.  Thus parsing technology plays almost no role in Lisp programs, and the construction of language processors is rarely an impediment to the rate of growth and change of large Lisp systems.  Finally, it is this very simplicity of syntax and semantics that is responsible for the burden and freedom borne by all Lisp programmers.  No Lisp program of any size beyond a few lines can be written without being saturated with discretionary functions.  Invent and fit; have fits and reinvent!  We toast the Lisp programmer who pens his thoughts within nests of parentheses.</p>
<quote source="Alan J. Perlis, New Haven, Connecticut"/>
</section>
</section>

<section title="License">
<p>Learn Scheme is an adaptation of <a href="http://mitpress.mit.edu/sicp/"><term>Structure and Interpretation of Computer Programs</term></a> by Harold Abelson and Gerald Jay Sussman with Julie Sussman, second edition, published by the MIT Press. (c) 1996 by Massachusetts Institute of Technology.</p>
<p>The content has been reformatted for the Web and made interactive by Jared Krinke. The Scheme interpreter is (c) 2015 Jared Krinke.</p>
<p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>
</section>
</back>

</content>
